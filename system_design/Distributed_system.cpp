# Distributed Systems - Complete Guide

## Table of Contents
1. [Introduction to Distributed Systems](#1-introduction-to-distributed-systems)
2. [Scaling Strategies](#2-scaling-strategies)
3. [Performance Metrics](#3-performance-metrics)
4. [System Communication](#4-system-communication)
5. [Load Balancing](#5-load-balancing)
6. [Caching](#6-caching)
7. [Content Delivery Networks (CDN)](#7-content-delivery-networks-cdn)
8. [Databases in Distributed Systems](#8-databases-in-distributed-systems)
9. [Data Sharding](#9-data-sharding)
10. [Replication](#10-replication)
11. [Consistency Models](#11-consistency-models)
12. [CAP Theorem](#12-cap-theorem)
13. [Message Queues](#13-message-queues)
14. [Event-Driven Architecture](#14-event-driven-architecture)
15. [Distributed Transactions](#15-distributed-transactions)
16. [Architecture Patterns](#16-architecture-patterns)
17. [Availability Patterns](#17-availability-patterns)

---

## 1. Introduction to Distributed Systems

### What Is a Distributed System?

A **Distributed System** is a collection of independent computers that work together to appear as a single unified system to the end user.

**Key Characteristics:**
- Multiple machines (nodes/servers) collaborate
- Users perceive it as ONE system
- Transparent operation (complexity is hidden)

### Real-World Analogy: Restaurant Kitchen

Imagine a busy restaurant:
- **Host** takes orders at the front
- **Chef** prepares the food in the kitchen
- **Sous chef** handles appetizers
- **Server** delivers food to tables

To the customer ‚Üí It feels like ONE seamless dining experience, not four separate people working independently.

**That's a distributed system!**

---

### Why Do We Need Distributed Systems?

| Need | Explanation | Real Example |
|------|-------------|--------------|
| **Scalability** | One machine cannot handle millions of concurrent users | Facebook serving 3 billion users |
| **Fault Tolerance** | System continues working even if components fail | Netflix streaming during server outages |
| **Performance** | Work is distributed and processed in parallel | Google Search returning results in milliseconds |
| **High Availability** | Service remains accessible 24/7 | Banking apps working around the clock |
| **Geographic Distribution** | Serve users from nearby locations | Amazon warehouses across continents |

---

### Core Features of Distributed Systems

| Feature | Description | Example |
|---------|-------------|---------|
| **Decentralization** | No single point of control | Bitcoin blockchain |
| **Replication** | Data exists in multiple copies | Your Gmail stored in 3+ data centers |
| **Consistency Management** | Keeping all copies synchronized | Bank account balance updates |
| **Network Communication** | Machines communicate via HTTP, RPC, gRPC | Microservices talking to each other |
| **Transparency** | Users don't see underlying complexity | YouTube feels like one platform |

### Pizza Chain Analogy

Think of Domino's Pizza:

**Problem:** One store in Mumbai cannot serve customers in Delhi, Bangalore, and Kolkata.

**Solution:** Open multiple branches across cities.

Each branch:
- ‚úÖ Follows the same menu
- ‚úÖ Uses the same pricing
- ‚úÖ Maintains the same quality standards
- ‚úÖ Shares the same brand identity

To customers ‚Üí It looks like ONE Domino's brand, but it's actually a distributed network of independent stores working together.

---

## 2. Scaling Strategies

### What Is Scalability?

**Scalability** means a system's ability to handle increased load by adding resources, with performance improving proportionally.

**Key Question:** If we double resources, does performance double?

### Types of Scaling

#### Vertical Scaling (Scaling UP) üèóÔ∏è

**Definition:** Increase the power of a single machine.

**What You Upgrade:**
- More powerful CPU
- Additional RAM
- Larger storage
- Better GPU

**Analogy:** Your phone is slow ‚Üí You buy a newer, more powerful phone instead of buying multiple phones.

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ No code changes needed
- ‚úÖ No distributed complexity

**Cons:**
- ‚ùå Hardware limits (you can't' infinitely upgrade)
- ‚ùå Expensive beyond a point
- ‚ùå Single point of failure

**Example:** Upgrading your laptop from 8GB RAM ‚Üí 32GB RAM

---

#### Horizontal Scaling (Scaling OUT) üîó

**Definition:** Add more machines instead of upgrading one.

**Analogy:** Instead of hiring one superhuman worker, hire 10 normal workers who collaborate.

**Pros:**
- ‚úÖ Nearly unlimited scaling potential
- ‚úÖ Better fault tolerance
- ‚úÖ Cost-effective at scale

**Cons:**
- ‚ùå Requires coordination between machines
- ‚ùå Data consistency challenges
- ‚ùå More complex architecture

**Example:** Netflix uses thousands of servers instead of one supercomputer.

**üí° Pro Tip:** Start with vertical scaling for simplicity, then move to horizontal scaling as your user base grows.

---

### Scalability Patterns

| Pattern | Description | Use Case |
|---------|-------------|----------|
| **Partitioning** | Split data across multiple databases | Large user base |
| **HTTPS Caching** | Store responses to avoid recomputation | Static content |(reverse proxy,CDN)
| **RDBMS Sharding** | Divide relational database into chunks | High write loads |
| **NoSQL** | Use non-relational databases for flexibility | Unstructured data |
| **Distributed Caching** | Share cache across servers | Session storage |
| **Concurrency** | Handle multiple requests simultaneously | Real-time apps |

---

## 3. Performance Metrics

### Latency ‚è±Ô∏è

**Definition:** The time it takes for a request to travel from client to server and back.

**Think of it as:** Network delay, ping, or lag.

**Components:**
```
Total Latency = Queue Latency + Server Processing Time
```

**Types of Latency:**

| Type | Description | Example |
|------|-------------|---------|
| **Network Latency** | Time to travel across network | 50ms to reach server |
| **Queue Latency** | Waiting time in request queue | 10 requests ahead of you |
| **Processing Latency** | Time server takes to compute | Database query takes 100ms |

**Performance Formula:**
```
Latency = Server Thread Time / Request Rate

If Latency < Rate Limit ‚Üí Good Performance ‚úÖ
If Latency > Rate Limit ‚Üí Queue builds up ‚ùå
```

**Example:**
- Sending a message on WhatsApp takes 200ms
- That 200ms = network travel + server processing + return journey

---

### Throughput üìä

**Definition:** The number of requests a system can handle per unit of time.

**Formula:**
```
Throughput = Requests Processed / Time Period
```

**Example:**
- A server processes 1,000 requests per second
- Throughput = 1,000 req/s

**Latency vs Throughput:**
- **Latency:** How fast one request completes
- **Throughput:** How many requests complete in a time window

**Analogy:**
- **Latency:** How fast one car crosses a bridge
- **Throughput:** How many cars cross the bridge per hour

---

### Rate Limiting üö¶

**Definition:** A control mechanism that limits the number of requests a client can make within a specific timeframe.

**Purpose:**
- Prevent system overload
- Protect against DDoS attacks
- Ensure fair resource distribution

**Common Limits:**
- 100 requests per minute
- 1,000 requests per hour
- 10,000 requests per day

**Example:**
Twitter API allows 300 requests per 15-minute window. If you exceed this, you get rate-limited and must wait.

---

## 4. System Communication

### DNS Lookup üåê

**Definition:** The process of translating human-readable domain names into IP addresses.

**Workflow:**
```
Client: "www.google.com" 
    ‚Üì
1. Check local cache
    ‚Üì
2. Ask DNS Resolver
    ‚Üì
3. Query Root Server: "Where is .com?"
    ‚Üì
4. Query TLD Server: "Where is google.com?"
    ‚Üì
5. Query Authoritative Server: "What is google.com's IP?"
    ‚Üì
Return: "142.250.192.46"
```

**DNS Resolution Types:**

| Type | Description |
|------|-------------|
| **Recursive** | DNS resolver does all work and returns final answer |
| **Iterative** | Resolver returns best known answer, client continues querying |

**Key DNS Components:**

| Component | Role |
|-----------|------|
| **Root Server** | Directs to TLD servers |
| **TLD Server** | Handles top-level domains (.com, .org) |
| **Authoritative Server** | Holds actual domain records (CNAME, A records) |
| **DNS Cache** | Stores recent lookups for faster access |

---

### Proxy Servers üîÑ

**Definition:** An intermediary server that sits between clients and backend servers, forwarding requests and responses.

**Types:**

#### Forward Proxy (Client-Side)
```
Client ‚Üí Forward Proxy ‚Üí Internet ‚Üí Server
```
**Use Cases:**
- Hide client identity
- Bypass geographic restrictions
- Content filtering

#### Reverse Proxy (Server-Side)
```
Client ‚Üí Internet ‚Üí Reverse Proxy ‚Üí Backend Servers
```
**Use Cases:**
- Load balancing
- SSL termination
- Caching
- Security (hide backend servers)

**Popular Tools:** Nginx, HAProxy, Apache

---

## 5. Load Balancing

### What Is Load Balancing?

**Definition:** The technique of distributing incoming network traffic across multiple servers to ensure no single server becomes overwhelmed.

**Core Concept:** Use **consistent hashing** to distribute load evenly.

**Formula:**
```
Load per server = Total Load / Number of Servers
Server assignment = Hash(User ID) % Number of Servers
```

**Simple Analogy:**

Imagine a bank with one cashier handling 100 customers.

**Problem:** Long queues, frustrated customers, overworked cashier.

**Solution:** Open 5 cashier counters and distribute customers evenly.

**That distribution mechanism = Load Balancer**

---

### Load Balancer Architecture

```
                    Internet
                       ‚Üì
              [Load Balancer]
                       ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì              ‚Üì              ‚Üì
   [Server A]     [Server B]     [Server C]
        ‚Üì              ‚Üì              ‚Üì
   [Database]     [Database]     [Database]
```

**Key Features:**
- SSL termination (handles HTTPS encryption)
- Session persistence (sticky sessions)
- Health checks (removes unhealthy servers)
- Cookie management

---

### Load Balancing Layers

#### Layer 4 Load Balancing (Transport Layer)

**Works at:** TCP/UDP level

**Decision Based On:**
- Source IP
- Destination IP
- Port numbers

**Characteristics:**
- ‚ö° Very fast
- üîí Cannot inspect request content
- üì¶ Works with any protocol

**Example:** Distributing based on client IP address.

---

#### Layer 7 Load Balancing (Application Layer)

**Works at:** HTTP/HTTPS level

**Decision Based On:**
- URL path
- HTTP headers
- Request body
- Cookies

**Characteristics:**
- üß† Smart routing decisions
- üîç Can inspect request content
- üéØ Route based on content type

**Example:**
```
/api/video/* ‚Üí Video Processing Servers
/api/payment/* ‚Üí High-Security Servers
/api/general/* ‚Üí General Servers
```

**üí° Layer 7 is more powerful but slightly slower than Layer 4.**

---

### Load Balancing Algorithms

#### 1Ô∏è‚É£ Round Robin

**How It Works:** Distribute requests sequentially in a circular manner.

```
Request Flow: A ‚Üí B ‚Üí C ‚Üí A ‚Üí B ‚Üí C ‚Üí A...
```

**Example:**

| Request | Assigned Server |
|---------|----------------|
| Request 1 | Server A |
| Request 2 | Server B |
| Request 3 | Server C |
| Request 4 | Server A |
| Request 5 | Server B |

**Best For:**
- ‚úÖ Servers with equal capacity
- ‚úÖ Similar request processing times
- ‚úÖ Simple setup

**Drawback:**
- ‚ùå Doesn't consider server load
- ‚ùå All servers treated equally

---

#### 2Ô∏è‚É£ Weighted Round Robin

**How It Works:** Servers with higher capacity receive proportionally more requests.

**Configuration:**
- Server A (16GB RAM): Weight = 3
- Server B (8GB RAM): Weight = 2
- Server C (4GB RAM): Weight = 1

**Distribution Pattern:**
```
A, A, A, B, B, C, A, A, A, B, B, C...
```

**Best For:**
- ‚úÖ Heterogeneous server hardware
- ‚úÖ Servers with different capabilities
- ‚úÖ API gateways with varying backend capacity

---

#### 3Ô∏è‚É£ Least Connections

**How It Works:** New requests go to the server with the fewest active connections.

**Example Scenario:**

| Server | Active Connections | Next Request? |
|--------|-------------------|---------------|
| Server A | 45 | ‚ùå |
| Server B | 12 | ‚úÖ (Winner) |
| Server C | 28 | ‚ùå |

**Formula:**
```
Selected Server = min(Active Connections across all servers)
```

**Best For:**
- ‚úÖ Chat applications
- ‚úÖ Video streaming
- ‚úÖ Long-lived connections
- ‚úÖ WebSocket connections

**Why It Works:**
Requests with varying duration (some take 1 second, others take 10 minutes) need dynamic balancing.

---

#### 4Ô∏è‚É£ IP Hash

**How It Works:** Use client's IP address to consistently route to the same server.

**Formula:**
```
Server = Hash(Client IP) % Number of Servers
```

**Example:**
```
Client IP: 192.168.1.100
Hash: 8547
Servers: 3
Assignment: 8547 % 3 = 0 ‚Üí Server A

Every time this client connects ‚Üí Always Server A
```

**Best For:**
- ‚úÖ Session-based applications
- ‚úÖ Shopping carts
- ‚úÖ Gaming servers
- ‚úÖ Sticky sessions

**Benefit:**
User's data stays on one server (session persistence without external storage).

---

#### 5Ô∏è‚É£ Least Response Time

**How It Works:** Route to the server with the fastest response time and fewest connections.

**Decision Formula:**
```
Score = Response Time √ó Active Connections
Selected Server = min(Score)
```

**Best For:**
- ‚úÖ Performance-critical applications
- ‚úÖ APIs with SLA requirements
- ‚úÖ Real-time services

---

#### 6Ô∏è‚É£ Health-Based (Failover)

**How It Works:** Continuously monitor server health and skip unhealthy servers.

**Health Check Methods:**
- Send heartbeat every 10 seconds
- Check HTTP response status
- Verify service availability

**Example:**

| Server | Status | Receives Traffic? |
|--------|--------|-------------------|
| Server A | üü¢ Healthy | ‚úÖ Yes |
| Server B | üî¥ Down | ‚ùå No |
| Server C | üü¢ Healthy | ‚úÖ Yes |

**Traffic Flow:**
```
Requests ‚Üí A, C, A, C, A, C...
(Server B is automatically excluded)
```

---

### Consistent Hashing

**Problem with Regular Hashing:**

When servers are added or removed, most keys get remapped:
```
Server 1, 2, 3 ‚Üí Add Server 4 ‚Üí 75% of keys change servers ‚ùå
```

**Consistent Hashing Solution:**

Maps both servers and keys onto a circular hash ring. When servers change, only a small portion of keys get remapped.

```
Visualization:

         Server A
            ‚Üì
     ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂
     ‚Üë             ‚Üë
Server C          Server B
     ‚óè             ‚óè

Keys between C and A ‚Üí Go to A
Keys between A and B ‚Üí Go to B
Keys between B and C ‚Üí Go to C
```

**Benefits:**
- ‚úÖ Minimal disruption when scaling
- ‚úÖ Only ~K/N keys remapped (K = total keys, N = servers)
- ‚úÖ Graceful degradation

**Use Cases:**
- Distributed caches (Redis, Memcached)
- CDN edge server selection
- Database sharding

---

## 6. Caching

### What Is Caching?

**Definition:** Storing frequently accessed data in fast temporary storage to avoid repeatedly fetching from slower sources.

**Simple Analogy:**

You're a student studying for exams.

**Without Cache:**
Every time you need a formula ‚Üí Walk to the library ‚Üí Find the book ‚Üí Read the formula ‚Üí Walk back.

**With Cache:**
You write important formulas on sticky notes on your desk. Now you just glance at the notes instantly.

**Those sticky notes = Cache**

---

### Why Cache in Distributed Systems?

| Challenge | How Cache Helps |
|-----------|-----------------|
| Slow database queries | Store results in memory |
| Network latency | Reduce trips to backend |
| Expensive computations | Store computed results |
| High traffic | Serve from cache instead of DB |

**Performance Impact:**
```
Database Query: ~100ms
Cache Hit: ~1ms

100x faster! ‚ö°
```

---

### Cache Hierarchy

```
Fastest ‚Üí CPU Cache (L1, L2, L3)
   ‚Üì
Application Memory Cache (In-Process)
   ‚Üì
Distributed Cache (Redis, Memcached)
   ‚Üì
Database Query Cache
   ‚Üì
Slowest ‚Üí Disk Storage
```

---

### Types of Caching

#### 1Ô∏è‚É£ Client-Side Caching

**Location:** User's browser or device

**What Gets Cached:**
- Images
- CSS/JavaScript files
- HTML pages
- API responses

**Example:**
When you revisit Amazon.com, your browser loads images from local storage instead of re-downloading.

---

#### 2Ô∏è‚É£ Application/In-Memory Cache

**Location:** Within the application's RAM

**Tools:** Guava Cache, Caffeine, In-memory maps

**Architecture:**
```
User Request ‚Üí Application Server ‚Üí Local Memory Cache
                                           ‚Üì
                                    (If miss ‚Üí Database)
```

**Pros:**
- ‚ö° Extremely fast (nanoseconds)
- üîí No network overhead

**Cons:**
- ‚ùå Not shared across servers
- ‚ùå Data inconsistency in multi-server setups
- ‚ùå Lost on server restart

**Best For:**
- Single-server applications
- Configuration data
- Session storage (single instance)

---

#### 3Ô∏è‚É£ Distributed Cache

**Location:** Separate cache servers shared by all application servers

**Popular Tools:** Redis, Memcached, Hazelcast

**Architecture:**
```
App Server A ‚îÄ‚îÄ‚îê
App Server B ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí [Redis Cluster]
App Server C ‚îÄ‚îÄ‚îò
```

**Pros:**
- ‚úÖ Shared across all servers
- ‚úÖ Consistent data
- ‚úÖ Highly scalable
- ‚úÖ Survives app server restarts

**Cons:**
- ‚ùå Network latency (still much faster than DB)
- ‚ùå Additional infrastructure

**Best For:**
- Multi-server applications
- Session management
- Real-time leaderboards
- Rate limiting counters

---

### Caching Strategies

#### Write-Through Cache

**Process:**
```
1. Write to Cache
2. Immediately write to Database
3. Return success
```

**Pros:**
- ‚úÖ Cache always consistent with DB
- ‚úÖ No data loss

**Cons:**
- ‚ùå Slower writes (double write penalty)

**Example:**
Banking transactions where accuracy is critical.

---

#### Write-Behind (Write-Back) Cache

**Process:**
```
1. Write to Cache
2. Return success immediately
3. Asynchronously write to Database (via queue)
```

**Pros:**
- ‚úÖ Very fast writes
- ‚úÖ Better write throughput

**Cons:**
- ‚ùå Risk of data loss if cache crashes
- ‚ùå Temporary inconsistency

**Example:**
Social media likes/reactions where slight delay is acceptable.

---

### Cache Eviction Policies

When cache is full, which items should be removed?

| Policy | Description | Use Case |
|--------|-------------|----------|
| **LRU (Least Recently Used)** | Remove items not accessed for longest time | General purpose |
| **LFU (Least Frequently Used)** | Remove items accessed least often | Content recommendation |
| **FIFO (First In, First Out)** | Remove oldest entries | Simple queues |
| **TTL (Time To Live)** | Remove after expiration time | Session data |
| **Random** | Remove random entries | When pattern unclear |

**Most Common:** LRU (balances recency and efficiency)

---

### Cache Invalidation

**The Hard Problem:** How do you know when cached data is stale?

**Strategies:**

#### 1. Time-Based (TTL)
```
Set cache expiry: 5 minutes
After 5 minutes ‚Üí Refresh from DB
```

#### 2. Event-Based
```
When database updates ‚Üí Invalidate related cache keys
```

#### 3. Manual
```
Admin triggers cache clear when needed
```

**Example:**
E-commerce product price:
- Cache for 1 hour (TTL)
- Invalidate immediately when price changes (Event)

---

### Cache Patterns

#### Cache-Aside (Lazy Loading)

```python
def get_user(user_id):
    # Try cache first
    user = cache.get(f"user:{user_id}")
    
    if user is None:  # Cache miss
        # Load from database
        user = database.query(user_id)
        # Store in cache
        cache.set(f"user:{user_id}", user, ttl=3600)
    
    return user
```

**When to use:** Read-heavy workloads

---

#### Read-Through Cache

```
Application ‚Üí Cache ‚Üí (Cache handles DB fetch if miss)
```

Cache automatically loads data from database on miss.

---

#### Refresh-Ahead

Proactively refresh cache before expiry.

**Example:**
Popular product pages refreshed every 30 minutes automatically.

---

## 7. Content Delivery Networks (CDN)

### What Is a CDN?

**Definition:** A globally distributed network of servers that delivers static content to users from the nearest geographic location.

**Purpose:** Reduce latency by serving content from nearby servers instead of distant origin servers.

---

### Ice Cream Delivery Analogy

**Problem:**

One ice cream factory in Delhi serves all of India.
- Customer in Chennai orders ice cream
- ‚ùå Takes 3 days to deliver
- ‚ùå Ice cream melts
- ‚ùå Customer angry

**Solution:**

Open ice cream storage centers in:
- Chennai
- Bangalore
- Mumbai
- Kolkata
- Hyderabad

Now customers get ice cream from the nearest center within hours!

**Those storage centers = CDN Edge Servers** üç¶

---

### CDN Architecture

```
        [Origin Server - USA]
                |
                ‚Üì
    [Global CDN Distribution]
         /      |      \
        ‚Üì       ‚Üì       ‚Üì
   [Asia     [Europe]  [South
    Edge]              America
                        Edge]
     ‚Üì         ‚Üì          ‚Üì
  Users in   Users in   Users in
   India    Germany     Brazil
```

---

### CDN Workflow

```
User in India requests image.jpg
        ‚Üì
1. DNS routes to nearest CDN edge (Mumbai)
        ‚Üì
2. Mumbai CDN checks cache
        ‚Üì
3a. Cache HIT ‚Üí Return image immediately ‚úÖ
        ‚Üì
3b. Cache MISS ‚Üí Fetch from origin ‚Üí Cache ‚Üí Return
        ‚Üì
4. Future requests served from cache (fast!)
```

---

### CDN Types

#### Push CDN

**Process:**
```
Developer uploads content ‚Üí CDN stores globally
Future user requests ‚Üí Served instantly from CDN
```

**Best For:**
- Static websites
- Infrequently changing content
- Small content volumes

**Example:** Deploying a marketing website

---

#### Pull CDN

**Process:**
```
User requests content ‚Üí CDN checks cache
Cache MISS ‚Üí CDN pulls from origin ‚Üí Caches ‚Üí Serves
Future requests ‚Üí Cache HIT (fast)
```

**Best For:**
- High traffic sites
- Large content volumes
- Frequently updated content

**Example:** YouTube videos, Netflix streams

---

### What CDNs Cache

| Content Type | Cacheable? | Why |
|--------------|-----------|-----|
| Images (PNG, JPG) | ‚úÖ Yes | Static |
| Videos (MP4) | ‚úÖ Yes | Large, static |
| CSS/JavaScript | ‚úÖ Yes | Rarely changes |
| HTML Pages | ‚ö†Ô∏è Sometimes | Depends on dynamic content |
| API Responses | ‚ö†Ô∏è Sometimes | If data doesn't change often |
| User-specific data | ‚ùå No | Dynamic, personalized |

---

### CDN Components

| Component | Description |
|-----------|-------------|
| **Origin Server** | Source of content |
| **CDN Entry Point** | Fetches content from origin |
| **Origin Shield** | Protects origin from traffic spikes |
| **Edge Servers** | Serve content to users |
| **CDN Footprint** | Geographic coverage areas |
| **CDN Selector** | Chooses best CDN (multi-CDN) |

---

### Cache Miss Optimization

**Problem:** High cache miss rate = Poor CDN performance

**Solutions:**

1. **Longer TTL:** Cache content for more time
2. **Predictive Prefetching:** Cache popular content before requested
3. **Origin Shield:** Reduce duplicate origin fetches

**Target Metrics:**
- Cache Hit Ratio: >90%
- Cache Miss Ratio: <10%

---

### Benefits of CDN

| Benefit | Impact |
|---------|--------|
| **Reduced Latency** | Content loads 10x faster |
| **Lower Bandwidth Costs** | Less origin server traffic |
| **Improved Availability** | Content served even if origin down |
| **DDoS Protection** | Edge servers absorb attacks |
| **Global Reach** | Serve users worldwide efficiently |

**Example:** Netflix uses CDN to serve 200+ million subscribers globally with minimal buffering.

IMPORTANT--
    In our architecture, all caching layers are accessed in the following order:
    CDN ‚Üí Proxy Server (forward/reverse proxy) ‚Üí Load Balancer ‚Üí External Cache (Redis) ‚Üí Database Cache ‚Üí Database.

    Redis acts as the primary external cache and refreshes data based on TTL. However, if all keys expire at the same time, Redis would attempt to regenerate everything simultaneously, which can overload the system.

    To prevent this ‚Äúcache stampede,‚Äù we use a locking mechanism so that only one request refreshes the expired data while the rest wait or serve stale content. This avoids system crashes.

    Whenever there is a cache miss, the system normally falls back to the database ‚Äî which is expensive because each lookup requires multiple network hops (e.g., 10 ms to Redis and 30 ms to DB for both request and response cycles).
    To optimize this, we maintain a lightweight bit-array (0s and 1s) between the cache and the database:

    1 ‚Üí the record definitely exists in the DB, so go fetch it.
    0 ‚Üí the record does not exist, so instantly return ‚ÄúNot Found‚Äù without hitting the DB.
    This drastically reduces unnecessary database lookups and improves latency under heavy load.
    
---

## 8. Databases in Distributed Systems

### Database Types

#### 1Ô∏è‚É£ Relational Databases (SQL)

**Structure:** Organized tables with rows and columns

**Examples:** MySQL, PostgreSQL, Oracle

**Best For:**
- Structured data
- Complex relationships
- ACID compliance needed

**Properties (ACID):**
- **Atomicity:** All or nothing
- **Consistency:** Valid state always
- **Isolation:** Transactions don't interfere
- **Durability:** Committed data persists

**Example Use Case:**
Banking system (accounts, transactions, balances)

---

#### 2Ô∏è‚É£ Document Databases

**Structure:** JSON-like documents

**Examples:** MongoDB, CouchDB

**Best For:**
- Unstructured/semi-structured data
- Flexible schemas
- Rapid development

**Example Document:**
```json
{
  "user_id": "12345",
  "name": "Priya",
  "orders": [
    {"item": "Laptop", "price": 50000},
    {"item": "Mouse", "price": 500}
  ]
}
```

**Query Example:**
```javascript
db.products.find({ qty: { $gt: 5 } })
```

---

#### 3Ô∏è‚É£ Columnar Databases

**Structure:** Data stored by columns instead of rows

**Examples:** Apache Cassandra, HBase

**Best For:**
- Analytics
- Large-scale data processing
- Time-series data

**Why Fast for Analytics:**
Reading one column from 1 billion rows is faster than reading all columns.

---

#### 4Ô∏è‚É£ Key-Value Databases

**Structure:** Simple map (key ‚Üí value)

**Examples:** Redis, DynamoDB, Riak

**Best For:**
- Caching
- Session storage
- Simple lookups

**Example:**
```
Key: "user:12345:session"
Value: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

---

#### 5Ô∏è‚É£ Graph Databases

**Structure:** Nodes and edges (relationships)

**Examples:** Neo4j, ArangoDB

**Best For:**
- Social networks
- Recommendation engines
- Fraud detection

**Example:**
```
// (Priya)-[FRIENDS_WITH]->(Raj)
// (Raj)-[LIKES]->(Pizza)
// (Priya)-[LIKES]->(Pizza)

Query: "Find friends who like the same food"
```

---

#### 6Ô∏è‚É£ Time-Series Databases

**Structure:** Optimized for timestamped data

**Examples:** InfluxDB, TimescaleDB

**Best For:**
- IoT sensor data
- Application metrics
- Stock prices

**Example:**
```
timestamp: 2025-12-10 14:30:00
metric: cpu_usage
value: 78.5%
```

---

#### 7Ô∏è‚É£ Vector Databases

**Structure:** Multi-dimensional vectors

**Examples:** Pinecone, Weaviate, Milvus

**Best For:**
- AI/ML applications
- Semantic search
- Recommendation systems

**How It Works:**
Text ‚Üí Vector embedding ‚Üí Store ‚Üí Find similar vectors

**Example:**
```
"Cat" ‚Üí [0.2, 0.8, 0.3, ...]
"Kitten" ‚Üí [0.21, 0.79, 0.31, ...] (similar vector)
```

---

## 9. Data Sharding

### What Is Sharding?

**Definition:** Splitting a large database into smaller, manageable pieces (shards) distributed across multiple servers.

**Why Needed:**
One database cannot handle:
- Billions of rows
- Millions of concurrent writes
- Terabytes of data

---

### Exam Paper Analogy

**Problem:**

One teacher checking 10,000 exam papers alone.
- ‚ùå Takes weeks
- ‚ùå Slow results
- ‚ùå Overworked teacher

**Solution:**

Divide papers among 5 teachers:

| Teacher | Papers | Time |
|---------|--------|------|
| Teacher A | Papers 1-2000 | 2 days |
| Teacher B | Papers 2001-4000 | 2 days |
| Teacher C | Papers 4001-6000 | 2 days |
| Teacher D | Papers 6001-8000 | 2 days |
| Teacher E | Papers 8001-10000 | 2 days |

**Result:** All papers checked in 2 days instead of weeks!

**This division = Sharding**

---

### Sharding Strategies

#### 1Ô∏è‚É£ Range-Based Sharding

**Split by:** Data ranges

**Example:**
```
Shard 1: User IDs 1 - 1,000,000
Shard 2: User IDs 1,000,001 - 2,000,000
Shard 3: User IDs 2,000,001 - 3,000,000
```

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ Easy to add shards

**Cons:**
- ‚ùå Uneven distribution (hotspots)
- ‚ùå Some shards busier than others

---

#### 2Ô∏è‚É£ Hash-Based Sharding

**Split by:** Hash function

**Formula:**
```
Shard = Hash(User_ID) % Number_of_Shards
```

**Example:**
```
User 12345 ‚Üí Hash = 87651 ‚Üí 87651 % 4 = 3 ‚Üí Shard 3
```

**Pros:**
- ‚úÖ Even distribution
- ‚úÖ No hotspots

**Cons:**
- ‚ùå Hard to add/remove shards (requires rehashing)
- ‚ùå Range queries difficult

---

#### 3Ô∏è‚É£ Geographic Sharding

**Split by:** User location

**Example:**
```
Shard Asia: Users in India, China, Japan
Shard Europe: Users in UK, Germany, France
Shard Americas: Users in USA, Brazil, Mexico
```

**Pros:**
- ‚úÖ Lower latency for users
- ‚úÖ Compliance with data residency laws

**Cons:**
- ‚ùå Uneven load distribution
- ‚ùå Complex cross-region queries

---

#### 4Ô∏è‚É£ Directory-Based Sharding

**How It Works:** Lookup table maps keys to shards

**Example:**
```
Lookup Service:
User 123 ‚Üí Shard A
User 456 ‚Üí Shard C
User 789 ‚Üí Shard B
```

**Pros:**
- ‚úÖ Flexible assignment
- ‚úÖ Easy rebalancing

**Cons:**
- ‚ùå Lookup service is single point of failure
- ‚ùå Extra network hop

---

### Sharding Challenges

| Challenge | Solution |
|-----------|----------|
| **Cross-Shard Queries** | Denormalization, scatter-gather |
| **Uneven Load** | Rebalancing, consistent hashing |
| **Shard Failure** | Replication, backup shards |
| **Transactions Across Shards** | Two-phase commit, sagas |

---

### When to Shard?

**Signals You Need Sharding:**
- ‚úÖ Database > 100GB and growing
- ‚úÖ Write throughput hitting limits
- ‚úÖ Single server CPU/RAM maxed out
- ‚úÖ Query performance degrading

**Start Simple:** Vertical scaling ‚Üí Read replicas ‚Üí Then sharding

---

## 10. Replication

### What Is Replication?

**Definition:** Creating and maintaining multiple copies of data across different servers for reliability and availability.

**Purpose:**
- ‚úÖ Fault tolerance (backup if server fails)
- ‚úÖ Higher read throughput (distribute reads)
- ‚úÖ Lower latency (serve from nearest replica)

---

### Instagram Photo Analogy

When you upload a photo to Instagram:

```
Your Photo ‚Üí Stored in:
    - Mumbai Server (Primary)
    - Singapore Server (Replica 1)
    - USA Server (Replica 2)
```

**Why?**
- If Mumbai server crashes ‚Üí Photo still available from Singapore/USA
- Users in Asia ‚Üí Served from Singapore (fast)
- Users in America ‚Üí Served from USA (fast)

---

### Replication Models

#### 1Ô∏è‚É£ Master-Slave (Primary-Replica)

**Architecture:**
```
        [Master] (Writes)
           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì      ‚Üì      ‚Üì
[Slave] [Slave] [Slave] (Reads)
```

**Process:**
1. All writes go to Master
2. Master replicates to Slaves
3. Reads distributed across Slaves

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ Scales reads effectively
- ‚úÖ Clear consistency model

**Cons:**
- ‚ùå Single point of failure (Master)
- ‚ùå Write bottleneck (one Master)

**Use Cases:**
- Read-heavy applications
- Analytics dashboards
- Content websites

---

#### 2Ô∏è‚É£ Master-Master (Multi-Master)

**Architecture:**
```
[Master A] ‚Üê‚Üí [Master B]
    ‚Üì              ‚Üì
Writes/Reads   Writes/Reads
```

**Process:**
1. Both nodes accept writes
2. Changes synchronized between masters
3. Reads from any master

**Pros:**
- ‚úÖ No single point of failure
- ‚úÖ Higher write throughput
- ‚úÖ Better availability

**Cons:**
- ‚ùå Complex conflict resolution
- ‚ùå Consistency challenges

**Conflict Example:**
```
User A updates name to "Priya" on Master A
User B updates name to "Priyanka" on Master B
Which is correct? (Needs resolution strategy)
```

---

#### 3Ô∏è‚É£ Peer-to-Peer Replication

**Architecture:**
```
[Node A] ‚Üê‚Üí [Node B]
    ‚Üï          ‚Üï
[Node D] ‚Üê‚Üí [Node C]
```

**Process:**
All nodes are equal, replicate to each other

**Example:** Cassandra's ring architecture

---

### Replication Strategies

#### Synchronous Replication

**Process:**
```
1. Write to Master
2. Wait for ALL replicas to confirm
3. Return success to client
```

**Characteristics:**
- ‚úÖ Strong consistency
- ‚ùå Slower writes (wait for all)
- ‚ùå Availability impact if replica down

**Use Cases:** Financial transactions, critical data

---

#### Asynchronous Replication

**Process:**
```
1. Write to Master
2. Return success immediately
3. Replicate to slaves in background
```

**Characteristics:**
- ‚úÖ Fast writes
- ‚úÖ Better availability
- ‚ùå Temporary inconsistency possible

**Use Cases:** Social media, analytics

---

#### Semi-Synchronous Replication

**Process:**
```
1. Write to Master
2. Wait for ONE replica to confirm
3. Return success
4. Other replicas update asynchronously
```

**Characteristics:**
- ‚öñÔ∏è Balance between consistency and performance

---

## 11. Consistency Models

### What Is Consistency?

**Definition:** The guarantee about when and how replicated data becomes synchronized across servers.

---

### WhatsApp Message Analogy

When you send a WhatsApp message:

1. **Sent** ‚úì (your device has it)
2. **Delivered** ‚úì‚úì (recipient's' device received it)
3. **Read** ‚úì‚úì (blue ticks - recipient saw it)

Updates arrive gradually ‚Üí This is **eventual consistency**

---

### Types of Consistency

#### 1Ô∏è‚É£ Strong Consistency

**Definition:** All reads return the most recent write immediately.

**Guarantee:** After write completes, all subsequent reads see the new value.

**Example:**
```
Time 0: Balance = $100
Time 1: Withdraw $20 (Balance = $80)
Time 2: Check balance ‚Üí MUST show $80
```

**How Achieved:** Uses TCP with acknowledgments (ACK)

**Cost:** Lower availability, higher latency

**Use Cases:**
- Banking systems
- Stock trading
- Inventory management

---

#### 2Ô∏è‚É£ Eventual Consistency

**Definition:** All replicas will eventually be consistent, but not immediately.

**Guarantee:** Given enough time without writes, all replicas converge to same value.

**Example:**
```
Time 0: Post a photo on Instagram
Time 1: You see it immediately
Time 2: Friend in USA sees it (1 second delay)
Time 3: Friend in Australia sees it (2 seconds delay)
Eventually: Everyone sees the same photo
```

**Cost:** Temporary inconsistency allowed

**Use Cases:**
- Social media likes
- View counts
- Comments
- DNS propagation

---

#### 3Ô∏è‚É£ Weak Consistency

**Definition:** No guarantee when updates will be visible.

**Example:** Live video streaming - some frames may be lost

---

#### 4Ô∏è‚É£ Read-After-Write Consistency

**Definition:** User sees their own writes immediately, but others may see stale data temporarily.

**Example:**
```
You post a comment ‚Üí You see it immediately
Others ‚Üí See it after 1-2 seconds
```

**Use Cases:** User-generated content

---

#### 5Ô∏è‚É£ Monotonic Read Consistency

**Definition:** Once you read a value, you'll never read an older value later.

**Example:**
```
Time 1: Read balance = $80
Time 2: Read balance = $70 or $80 or $90 ‚úÖ
Time 2: Read balance = $75 (older) ‚ùå VIOLATION
```

---

### Two Generals' Problem

**Scenario:** Two armies need to coordinate attack, but messengers can be lost.

**Problem:**
```
General A sends: "Attack at dawn"
General B receives, sends: "Acknowledged"
General A receives acknowledgment

But... How does B know A received the acknowledgment?
Infinite ACK loop!
```

**Lesson:** Perfect consistency in distributed systems with unreliable networks is impossible.

**Real-World Impact:** This is why we use TCP (with ACKs) for strong consistency, accepting the availability trade-off.

---

## 12. CAP Theorem

### What Is CAP Theorem?

**CAP Theorem** states: In a distributed system with network partitions, you can only guarantee 2 out of 3 properties.

| Letter | Property |
|--------|----------|
| **C** | Consistency - All nodes see the same data |
| **A** | Availability - System always responds |
| **P** | Partition Tolerance - Works despite network failures |

---

### The Critical Insight

**Partition Tolerance is MANDATORY** in distributed systems (networks can always fail).

Therefore, the real choice is:
- **CP** (Consistency + Partition Tolerance)
- **AP** (Availability + Partition Tolerance)

---

### Restaurant Analogy

Imagine a restaurant chain with two branches:

**Network Partition:** Phone lines between branches are down.

**CP Choice (Consistency Priority):**
```
Branch A gets new menu update
Branch B can't sync due to network failure
Result: Branch B closes temporarily (unavailable)
        Until sync completes, ensure consistency
```

**AP Choice (Availability Priority):**
```
Branch A gets new menu update
Branch B can't sync due to network failure
Result: Both branches stay open (available)
        But serving different menus (inconsistent)
```

---

### CP Systems (Consistency + Partition Tolerance)

**Behavior:** System may refuse requests to maintain consistency.

**Examples:**
- **Banking systems** - Can't show wrong balance
- **HBase** - Consistent distributed database
- **MongoDB (strong consistency mode)** - Ensures data accuracy
- **Zookeeper** - Distributed coordination

**Trade-off:** Some requests may fail or timeout during network issues.

**When to Choose CP:**
- Correctness is critical
- Can tolerate brief downtime
- Financial transactions
- Inventory management

---

### AP Systems (Availability + Partition Tolerance)

**Behavior:** System always responds, even with potentially stale data.

**Examples:**
- **Instagram likes** - Slight delay is acceptable
- **Cassandra** - Always available
- **DynamoDB** - High availability
- **Couchbase** - Distributed JSON database

**Trade-off:** Data may be temporarily inconsistent.

**When to Choose AP:**
- Uptime is critical
- Can tolerate eventual consistency
- Social media
- Shopping carts (with conflict resolution)
- Analytics dashboards

---

### CAP in Practice

**Reality Check:** Most systems offer tunable consistency.

**Example: Cassandra**
```
Write with consistency level:
- ONE (fast, less consistent - AP)
- QUORUM (balanced)
- ALL (slow, consistent - CP)
```

---

## 13. Message Queues

### What Is a Message Queue?

**Definition:** A system that stores messages temporarily and delivers them asynchronously between services.

**Key Concept:** Decouples producers from consumers.

Service A ‚Üí sends message ‚Üí Service B receives it(only communication no works as, it does it from sender to reviver no work)


---

### Post Office Analogy

**Without Message Queue:**
```
You write a letter ‚Üí Must personally deliver to recipient
- What if recipient is busy?
- What if you're busy when they're free?
```

**With Message Queue:**
```
You write letter ‚Üí Drop in mailbox ‚Üí Post office holds it ‚Üí Delivers when recipient available
```

**The mailbox = Message Queue**

---

### Architecture

```
[Producer Service] ‚Üí [Message Queue] ‚Üí [Consumer Service]
                         ‚Üì
                   (Stores messages)
                   (Guarantees delivery)
```

---

### Key Components

| Component | Description | Example |
|-----------|-------------|---------|
| **Producer** | Sends messages | Order service |
| **Queue/Topic** | Stores messages | RabbitMQ queue |
| **Consumer** | Processes messages | Email service |
| **Broker** | Manages queue system | Kafka, RabbitMQ |
| **Acknowledgment** | Confirms processing | ACK signal |
| **Dead Letter Queue** | Failed message storage | Error handling |

---

### Message Queue Workflow

```
1. Order Service creates order ‚Üí Sends message to queue
2. Queue stores: {"order_id": 123, "user": "Priya"}
3. Email Service pulls message from queue
4. Email Service sends confirmation email
5. Email Service sends ACK to queue
6. Queue deletes message (successfully processed)
```

**If Step 4 Fails:**
```
- No ACK sent
- Message stays in queue
- Retry after timeout
- If repeated failures ‚Üí Move to Dead Letter Queue
```

---

### Queue Monitoring Pattern

**Notifier Component:** Monitors system health

```
Every 10 seconds:
1. Notifier sends heartbeat to all servers
2. Servers respond with health status
3. If server doesn't' respond ‚Üí Mark as dead
4. Check database: Was work completed?
5. If not completed ‚Üí Re-queue task
6. Load balancer assigns to healthy server
```

---

### Popular Message Queue Systems

#### 1Ô∏è‚É£ RabbitMQ

**Best For:** Traditional message queuing

**Features:**
- Multiple exchange types
- Routing rules
- Message acknowledgments

---

#### 2Ô∏è‚É£ Apache Kafka

**Best For:** High-throughput event streaming

**Features:**
- Distributed log
- Millions of messages/second
- Data retention
- Event replay capability

**Architecture:**
```
Producers ‚Üí Kafka Topics (Partitioned) ‚Üí Consumer Groups
```

**Use Cases:**
- Event sourcing
- Log aggregation
- Real-time analytics

---

#### 3Ô∏è‚É£ AWS SQS

**Best For:** Managed queue service

**Features:**
- Fully managed
- Auto-scaling
- Serverless integration

---

### Benefits of Message Queues

| Benefit | Explanation |
|---------|-------------|
| **Decoupling** | Services don't need to know about each other |
| **Scalability** | Add more consumers to handle load |
| **Reliability** | Messages not lost if consumer is down |
| **Async Processing** | Producer doesn't wait for consumer |
| **Load Leveling** | Smooth traffic spikes |

---

### Real-World Example: E-Commerce Order

```
User places order:

1. Order Service ‚Üí Queue: {"order": {...}, "action": "process"}
2. Payment Service ‚Üê Consumes from queue
3. Payment Service ‚Üí Queue: {"order": {...}, "action": "ship"}
4. Shipping Service ‚Üê Consumes from queue
5. Shipping Service ‚Üí Queue: {"order": {...}, "action": "notify"}
6. Email Service ‚Üê Consumes from queue

Each service independent, scalable, reliable!
```
Task queues:-
    App ‚Üí sends task ‚Üí Celery worker executes it(Ask someone to DO something.)
    when ever a user makes a request the task queue handles it in background to return a result
    eg:->when a user orders a pizza(message queue) then the cheif prepares the pizza(task queues)
        2)write a email(task queue will do it in background)

    Tools:-> celery(django),RQ
    celery works->
    user->application(request bank statements)->taskqueue(redis,kafka)<-->celery(performs the task in lifo and return it to the DB as request in background)
---

## 14. Event-Driven Architecture

### What Is Event-Driven Architecture (EDA)?

**Definition:** Systems communicate by publishing and subscribing to events rather than direct requests.

**Key Concept:** "Something happened" ‚Üí Multiple services react independently.

---

### School Bell Analogy

Imagine a school bell üîî rings at 3 PM.

**The bell is the event.**

**Reactions:**
- Students: Stop studying, pack bags
- Teachers: End lectures
- Peon: Opens main gate
- Canteen: Starts preparing snacks
- Security: Manages exit traffic

**Important:** The bell didn't' tell each person what to do. It simply announced an event. Everyone decided their own response.

**This is Event-Driven Architecture!**

---

### Traditional vs Event-Driven

#### Traditional (Request-Response)
```
Order Service ‚Üí Calls Payment Service
            ‚Üí Calls Shipping Service
            ‚Üí Calls Email Service

(Tight coupling, sequential)
```

#### Event-Driven
```
Order Service ‚Üí Publishes "OrderPlaced" event
    ‚Üì
Payment Service ‚Üê Subscribes, processes payment
Shipping Service ‚Üê Subscribes, prepares shipment
Email Service ‚Üê Subscribes, sends confirmation
Analytics Service ‚Üê Subscribes, logs data

(Loose coupling, parallel)
```

---

### Core Components

| Component | Role |
|-----------|------|
| **Event Producer** | Publishes events |
| **Event Broker** | Routes events (Kafka, RabbitMQ, SNS) |
| **Event Consumer** | Subscribes and reacts to events |
| **Event** | Data about something that happened |

---

### Event Structure

```json
{
  "event_type": "OrderPlaced",
  "timestamp": "2025-12-10T14:30:00Z",
  "data": {
    "order_id": "12345",
    "user_id": "98765",
    "amount": 2500,
    "items": [...]
  }
}
```

---

### Publisher-Subscriber Pattern

```
         [Event Broker]
              ‚Üì
        [Topic: Orders]
         /    |    \
        ‚Üì     ‚Üì     ‚Üì
    [Sub 1] [Sub 2] [Sub 3]
   Payment  Shipping  Email
```

**Key Features:**
- Publishers don't know subscribers
- Subscribers don't know publishers
- Dynamic subscription

---

### Benefits of EDA

| Benefit | Explanation |
|---------|-------------|
| **Loose Coupling** | Services independent |
| **Scalability** | Add subscribers easily |
| **Resilience** | One service failure doesn't break others |
| **Real-time Processing** | React to events instantly |
| **Flexibility** | Add new features without changing existing code |

---

### Event-Driven Use Cases

**E-Commerce:**
```
OrderPlaced event ‚Üí
  - Payment: Charge card
  - Inventory: Reserve items
  - Shipping: Generate label
  - Analytics: Track sale
  - Email: Send confirmation
```

**Social Media:**
```
PostCreated event ‚Üí
  - Newsfeed: Add to followers' feeds
  - Notification: Alert tagged users
  - Moderation: Check for violations
  - Analytics: Track engagement
```

---

## 15. Distributed Transactions

### The Challenge

**Problem:** How do you update data across multiple services/databases atomically?

**Example:**
```
Transfer $100 from Account A to Account B

Step 1: Deduct $100 from A
Step 2: Add $100 to B

What if Step 1 succeeds but Step 2 fails?
Money disappears! üí∏
```

---

### Two-Phase Commit (2PC)

**Definition:** A distributed transaction protocol ensuring all participants commit or all abort.

---

#### Phase 1: Prepare (Voting)

```
Coordinator: "Everyone ready to commit?"
    ‚Üì
Participant A: "Yes, ready" ‚úÖ
Participant B: "Yes, ready" ‚úÖ
Participant C: "No, error!" ‚ùå
```

---

#### Phase 2: Commit/Abort

```
If all voted YES:
    Coordinator: "Everyone commit!"
    All participants commit ‚úÖ

If any voted NO:
    Coordinator: "Everyone abort!"
    All participants rollback ‚ùå
```

---

### 2PC Workflow

```
1. Application starts transaction
2. Coordinator asks all participants to prepare
3. Participants lock resources, vote YES or NO
4. If all YES ‚Üí Coordinator commits
5. If any NO ‚Üí Coordinator aborts
6. Participants execute commit/abort
7. Release locks
```

---

### 2PC Issues

| Issue | Problem |
|-------|---------|
| **Blocking** | If coordinator fails, participants wait forever |
| **Performance** | Locks held for entire 2PC duration |
| **Not Partition-Tolerant** | Network split causes issues |

**Verdict:** 2PC prioritizes consistency over availability (CP system)

---

### Saga Pattern

**Definition:** Break long transaction into smaller local transactions with compensating actions.

**Philosophy:** Instead of locking, use compensation if something fails.

---

### Saga Example: E-Commerce Order

#### Forward Flow (Happy Path)
```
1. Order Service: Create order ‚úÖ
2. Payment Service: Charge card ‚úÖ
3. Inventory Service: Reserve items ‚úÖ
4. Shipping Service: Schedule delivery ‚úÖ

Success! üéâ
```

#### Compensation Flow (Failure)
```
1. Order Service: Create order ‚úÖ
2. Payment Service: Charge card ‚úÖ
3. Inventory Service: Reserve items ‚ùå FAILS

Compensate:
3. Inventory Service: (no action needed)
2. Payment Service: Refund card üí≥
1. Order Service: Cancel order ‚ùå

Saga rolled back!
```

---

### Saga Characteristics

**Compensating Actions:**

| Service | Transaction | Compensating Action |
|---------|------------|---------------------|
| Order | Create order | Cancel order |
| Payment | Charge card | Refund payment |
| Inventory | Reserve items | Release items |
| Shipping | Schedule delivery | Cancel shipment |

**Pros:**
- ‚úÖ Better availability (no long locks)
- ‚úÖ Works with network partitions
- ‚úÖ Scales better

**Cons:**
- ‚ùå Eventual consistency
- ‚ùå Complex compensation logic
- ‚ùå Some actions hard to compensate (emails sent)

---

### When to Use What?

| Scenario | Use |
|----------|-----|
| Banking transactions | 2PC |
| Microservices architecture | Saga |
| Need strong consistency | 2PC |
| Need high availability | Saga |
| Compensations possible | Saga |
| Compensations impossible | 2PC |

---

## 16. Architecture Patterns

### Monolithic Architecture

**Definition:** Entire application built as a single unit.

**Components in One Codebase:**
- User Interface
- Business Logic
- Database Access
- Authentication
- Payments
- Notifications

---

### Restaurant Analogy: Monolith

Imagine a restaurant where everyone works in ONE ROOM:
- Chef cooking
- Cashier billing
- Waiter serving
- Cleaner mopping
- Delivery person waiting

**Problems:**
- ‚ùå If chef is slow ‚Üí Everyone affected
- ‚ùå Can't scale kitchen separately
- ‚ùå Any change requires restarting entire restaurant
- ‚ùå One bug crashes entire operation

---

### Monolith Characteristics

**Pros:**
- ‚úÖ Simple to develop initially
- ‚úÖ Easy to test (everything together)
- ‚úÖ Straightforward deployment
- ‚úÖ No network overhead

**Cons:**
- ‚ùå Hard to scale specific parts
- ‚ùå Slow deployments (must deploy everything)
- ‚ùå Technology lock-in
- ‚ùå One bug can crash entire app

**Best For:**
- Small teams
- Simple applications
- MVPs and prototypes

---

### Microservices Architecture

**Definition:** Application split into small, independent services that communicate over a network.

**Each Service:**
- Own codebase
- Own database (often)
- Own deployment
- Own team

---

### Restaurant Analogy: Microservices

Same restaurant, different approach:
- **Kitchen** (separate building) - Food Service
- **Cashier** (front desk) - Payment Service
- **Delivery** (external team) - Logistics Service
- **Cleaning** (contract staff) - Maintenance Service

**Benefits:**
- ‚úÖ If delivery fails ‚Üí Kitchen still works
- ‚úÖ Hire more delivery staff without affecting kitchen
- ‚úÖ Each team specialized and independent

---

### Microservices Characteristics

**Pros:**
- ‚úÖ Independent scaling (scale what you need)
- ‚úÖ Technology flexibility (different languages per service)
- ‚úÖ Faster deployments (deploy one service)
- ‚úÖ Fault isolation (one service down doesn't' crash all)
- ‚úÖ Team autonomy

**Cons:**
- ‚ùå Increased complexity
- ‚ùå Network latency
- ‚ùå Distributed debugging challenges
- ‚ùå Data consistency issues
- ‚ùå Operational overhead (monitoring, logging)

---

### Comparison Table

| Aspect | Monolith | Microservices |
|--------|----------|---------------|
| **Codebase** | Single | Multiple |
| **Deployment** | All at once | Independent |
| **Scaling** | Entire app | Per service |
| **Technology** | One stack | Mix of technologies |
| **Database** | Shared | Per service (typically) |
| **Team Structure** | One team | Multiple teams |
| **Communication** | Function calls | REST/gRPC/Kafka |
| **Testing** | Easier | Complex |
| **Infrastructure** | Simple | Complex (Kubernetes) |

---

### Technology Examples

**Monolith Stack:**
- Django (Python)
- Ruby on Rails
- Laravel (PHP)
- Single PostgreSQL database

**Microservices Stack:**
- Service A: Spring Boot (Java)
- Service B: FastAPI (Python)
- Service C: Go microservice
- Service D: Node.js
- Databases: MongoDB, Redis, PostgreSQL, Cassandra
- Communication: REST, gRPC, Kafka
- Orchestration: Kubernetes, Docker

---

## 17. Availability Patterns

### What Is Availability?

**Definition:** The percentage of time a system is operational and accessible.

**Measurement:**
```
Availability = (Uptime / Total Time) √ó 100%
```

---

### Availability Levels

| Level | Uptime % | Downtime/Year |
|-------|----------|---------------|
| 99% (Two nines) | 99% | 3.65 days |
| 99.9% (Three nines) | 99.9% | 8.76 hours |
| 99.99% (Four nines) | 99.99% | 52.6 minutes |
| 99.999% (Five nines) | 99.999% | 5.26 minutes |

**Note:** Each additional nine is exponentially harder and more expensive to achieve.

---

### Failover Patterns

#### Active-Passive Failover

**Setup:**
- **Active Server:** Handles all requests
- **Passive Server:** Standby mode, receives heartbeats

```
[Active Server] ‚Üí Sends heartbeat ‚Üí [Passive Server]
      ‚Üì                                    ‚Üì
   Handles                            (Standby mode)
   traffic
```

**Failover Process:**
```
1. Active server crashes
2. Passive server detects missed heartbeats
3. Passive promotes itself to active
4. Starts handling traffic
```

**Pros:**
- ‚úÖ Simple setup
- ‚úÖ Resource efficient (passive server idle)

**Cons:**
- ‚ùå Wasted resources (passive does nothing)
- ‚ùå Failover takes time (30-60 seconds)

---

#### Active-Active Failover

**Setup:**
Both servers handle traffic simultaneously.

```
[Load Balancer]
    /        \
   ‚Üì          ‚Üì
[Server A]  [Server B]
  Active     Active
```

**Failover Process:**
```
1. Server A crashes
2. Load balancer detects failure
3. Routes all traffic to Server B
4. No promotion needed
```

**Pros:**
- ‚úÖ Better resource utilization
- ‚úÖ Faster failover (instant)
- ‚úÖ Higher throughput

**Cons:**
- ‚ùå More complex setup
- ‚ùå Data synchronization challenges

---

### Replication for High Availability

#### Master-Slave Replication

```
[Master Database]
      ‚Üì (Replication)
  [Slave 1]  [Slave 2]
```

**Availability Benefit:**
If master fails ‚Üí Promote slave to master

---

#### Master-Master Replication

```
[Master A] ‚Üê‚Üí [Master B]
```

**Availability Benefit:**
Either master can handle requests anytime

---

### Heartbeat Mechanism

**Definition:** Regular "I'm alive" signals between servers.

```
Every 10 seconds:
Active Server ‚Üí "Still alive" ‚Üí Passive Server

If 3 consecutive heartbeats missed:
Passive Server ‚Üí Assumes active is dead ‚Üí Takes over
```

---

### Geographic Redundancy

**Multi-Region Deployment:**

```
      [Global Load Balancer]
         /        |        \
        ‚Üì         ‚Üì         ‚Üì
   [US Region] [EU Region] [Asia Region]
```

**Benefits:**
- ‚úÖ Survives regional outages
- ‚úÖ Lower latency (serve from nearby region)
- ‚úÖ Compliance with data residency

**Example:** Netflix operates in 190+ countries with regional deployments.

---

## Summary: Distributed Systems Journey

### Design Progression

1. **Start Simple:** Monolithic app on single server
2. **Scale Vertically:** Upgrade server hardware
3. **Add Caching:** Reduce database load
4. **Add CDN:** Serve static content globally
5. **Scale Horizontally:** Add more servers + load balancer
6. **Add Replication:** Database read replicas
7. **Introduce Sharding:** Split database across servers
8. **Use Message Queues:** Decouple services
9. **Adopt Microservices:** Split into independent services
10. **Implement Monitoring:** Track health, metrics, logs

---

### Key Principles

‚úÖ **Start simple, evolve as needed**
‚úÖ **Measure before optimizing**
‚úÖ **Trade-offs are inevitable (CAP theorem)**
‚úÖ **Design for failure**
‚úÖ **Automate everything**
‚úÖ **Monitor, log, alert**

---

### Final Thoughts

Building distributed systems is about making **informed trade-offs**:
- Consistency vs Availability
- Latency vs Throughput
- Simplicity vs Scalability
- Cost vs Performance

**There's no perfect design‚Äîonly the right design for your specific needs.**

---

**End of Complete Guide** ‚úÖ